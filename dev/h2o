    TOP_FEATURES = 5
    
    h2o.get_auc <- function(model, data, response) {
      pred <- h2o.predict(model, data)[,3]
      perf <- h2o.performance(pred, data[,response])
      perf@model$auc
    }
    
    h2o.varimp <- function(algo, model) {
      if (identical(algo, h2o.glm)) {
        varimp <- paste(names(sort(abs(model@model$normalized_coefficients), TRUE))[1:TOP_FEATURES], collapse = ",", sep = ",")
      } else if (identical(algo, h2o.randomForest) || identical(algo, h2o.deeplearning)) {
        varimp <- paste(names(sort(model@model$varimp[1,], TRUE))[1:TOP_FEATURES], collapse = ",", sep = ",")
      } else if (identical(algo, h2o.gbm)) {
        varimp <- paste(rownames(model@model$varimp)[1:TOP_FEATURES], collapse = ",", sep = ",")
      }
      varimp
    }
    
    h2o.validate <- function(t0, model, modeltype, validation, response, varimp) {
      elapsed_seconds <- as.numeric(Sys.time()) - as.numeric(t0)
      modelkey <- model@key
      type <- modeltype
      auc <- h2o.get_auc(model, validation, response)
      result <- list(list(model, modeltype, response, elapsed_seconds, auc, varimp))
      names(result) <- model@key
      return(result)
    }
    
    h2o.fit <- function(algo, data, args) {
      t0 <- Sys.time()
      predictors <- data$x
      response <- data$y
      train <- data$train
      valid <- data$valid
      nfolds <- data$nfolds
      if (nfolds >= 0) {
        model <- do.call(algo, modifyList(list(x=predictors, y=response, data=train, nfolds=nfolds), args))
      } else {
        model <- do.call(algo, modifyList(list(x=predictors, y=response, data=train), args))
      }
      if (.hasSlot(model,"sumtable")) {
        model <- model@model[[1]]
      }
      return(h2o.validate(t0, model, as.character(substitute(algo)), valid, response, h2o.varimp(algo, model)))
    }
    
    h2o.selectModel <- function(x) {
      c(model_key = x[[1]]@key,
        model_type = x[[2]],
        train_auc = as.numeric(x[[1]]@model$auc),
        validation_auc = as.numeric(x[[5]]),
        important_feat = x[[6]],
        tuning_time_s = as.numeric(as.character(x[[4]])))
    }
    
    h2o.leaderBoard <- function(models, test_hex, response) {
      model.list <- as.data.frame(t(as.data.frame(lapply(models, h2o.selectModel))))
      model.list$train_auc <- as.numeric(as.character(model.list$train_auc))
      model.list$validation_auc <- as.numeric(as.character(model.list$validation_auc))
      
      #### sort the models by AUC from worst to best
      models.sort.by.auc <- model.list[with(model.list, order(validation_auc)),-1]
      models.sort.by.auc <- models.sort.by.auc[rev(rownames(models.sort.by.auc)),]
    
      #### convert the `auc` and `tuning_time` columns into numerics
      models.sort.by.auc$train_auc       <- as.numeric(as.character(models.sort.by.auc$train_auc))
      models.sort.by.auc$validation_auc  <- as.numeric(as.character(models.sort.by.auc$validation_auc))
      models.sort.by.auc$tuning_time_s   <- as.numeric(as.character(models.sort.by.auc$tuning_time_s))
      
      #### display the frame
      print(models.sort.by.auc)
      
      #### score the best model on the test data
      best_model <- h2o.getModel(h2oServer, rownames(models.sort.by.auc)[1])
      preds <- h2o.predict(best_model, test_hex)
      test_auc <- h2o.get_auc(best_model, test_hex, response)
      
      cat(paste(" -------------------------------\n",
                "Best Model Performance On Final Testing Data:", "\n",
                "AUC = ", round(test_auc,6), "\n",
                "--------------------------------\n"))
      
      cat(paste(" =---------Summary------------=\n",
                "Best model type: ", models.sort.by.auc[1,]$model_type, "\n",
                "Best model AUC on test: ", round(test_auc,6), "\n",
                "Top", TOP_FEATURES, "important features: ", models.sort.by.auc[1,]$important_feat, "\n",
                "Model training time (incl. tuning, grid search): ", round(models.sort.by.auc[1,]$tuning_time_s,6), "seconds \n",
                "Training data rows: ", nrow(train_hex), "\n",
                "Training data cols: ", ncol(train_hex), "\n",
                "Validation data rows: ", nrow(valid_hex), "\n",
                "=----------------------------=\n"))
      best_model
    }

library(h2o)
h2oServer <- h2o.init(nthreads=-1)


homedir <- wd
TRAIN = "train.csv"
data_hex <- h2o.importFile(h2oServer, path = paste0(homedir,TRAIN), header = T, sep = ',', key = 'data_hex')

data_hex <- h2o.rebalance(data_hex, chunks=64, key='data_hex.rebalanced')

random <- h2o.runif(data_hex, seed = 123456789)
train_hex <- h2o.assign(data_hex[random < .8,], "train_hex")
valid_hex <- h2o.assign(data_hex[random >= .8 & random < .9,], "valid_hex")
test_hex  <- h2o.assign(data_hex[random >= .9,], "test_hex")
h2o.rm(h2oServer, grep(pattern = "Last.value", x = h2o.ls(h2oServer)$Key, value = TRUE))

response = 2
low_level_predictors = c(3:11)
low_and_high_level_predictors = c("Sex","Age","Pclass")

N_FOLDS=2

for (preds in list(low_level_predictors, low_and_high_level_predictors)) {
  data = list(x=preds, y=response, train=train_hex, valid=valid_hex, nfolds=N_FOLDS)

  models <- c(
    h2o.fit(h2o.glm, data,
            list(family="binomial", variable_importances=T, lambda=c(1e-5,1e-4), use_all_factor_levels=T)),
    h2o.fit(h2o.randomForest, data,
            list(type="fast", importance=TRUE, ntree=c(5), depth=c(5,10))),
    h2o.fit(h2o.randomForest, data,
            list(type="BigData", importance=TRUE, ntree=c(5), depth=c(5,10))),
    h2o.fit(h2o.gbm, data,
            list(importance=TRUE, n.tree=c(10), interaction.depth=c(2,5))),
    h2o.fit(h2o.deeplearning, data,
            list(variable_importances=T, l1=c(1e-5), epochs=1, hidden=list(c(10,10,10), c(100,100))))
  )
  best_model <- h2o.leaderBoard(models, test_hex, response)
  h2o.rm(h2oServer, grep(pattern = "Last.value", x = h2o.ls(h2oServer)$Key, value = TRUE))
}


h2o.deeplearning(x=low_level_predictors, y=response, activation="RectifierWithDropout", data=train_hex,
                 validation=valid_hex, input_dropout_ratio=0, hidden_dropout_ratios=c(0.2,0.1,0.1,0),
                 l1=1e-5, l2=1e-5, epochs=20, hidden=c(200,200,200,200))



evaluate_deep_neural_network <- function(object, train_hex, valid_hex, test_hex){
  #Function code
  model <- h2o.deeplearning(
  	    x=3:12
  	  , y=2
  	  ,activation="RectifierWithDropout"
  	  ,data=train_hex
  	  ,validation=valid_hex
  	  ,input_dropout_ratio=0
  	  ,hidden_dropout_ratios=c(0.2,0.1,0.1,0)
  	  ,l1=1e-5
  	  ,l2=1e-5
  	  ,epochs= 10
  	  ,hidden=c(object@chr$hidden1,object@chr$hidden2,object@chr$hidden3)
  ) 


  h2o.get_auc <- function(model, data, response = 2) {
      pred <- h2o.predict(model, data)[,3]
      perf <- h2o.performance(pred, data[,response])
      perf@model$auc
    }

  score <- h2o.get_auc(model, test_hex, )
  #Retrun Score
  return(score)

}

chr <- list(
   hidden1 = 100
  ,hidden2 = 100
  ,hidden3 = 100
)
genes.class <- list(
  "integer"
)
genes.range <- list(
 c(1,100)
)

waRRior.machinelearning.geneticalgorithm.run(
   chr.init = chr
  ,genes.class = genes.class
  ,genes.range = genes.range
  ,train.data = train_hex
  ,validation.data = valid_hex
  ,test.data = test_hex
  ,evaluate.function = evaluate_deep_neural_network
  ,verbose = F
  ,plot.logscale = F
)
